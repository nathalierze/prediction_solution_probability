{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "path = '#'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import survey data set\n",
    "\n",
    "infile = open(path+'umfrage2020only.pkl','rb')\n",
    "umfrage = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "# Deutschnote: bereinigen\n",
    "umfrage['deutschnote'] = umfrage['deutschnote'].replace(['Es gab keine Noten', np.nan],0)\n",
    "umfrage['deutschnote'] = umfrage['deutschnote'].astype('int')\n",
    "# Erstschrift: deutsch=1, nicht deutsch=0\n",
    "umfrage['ErstSchrift'] = umfrage['ErstSchrift'].replace([np.nan,'Russisch','Serbisch','Englisch','Italienisch','Vietnamesisch','Persisch','Kroatisch','Polnisch','Kurdisch','Arabisch','Griechisch','Spanisch','Niederl&auml;ndisch','T&uuml;rkisch','eine andere, und zwar...'],0)\n",
    "umfrage['ErstSchrift'] = umfrage['ErstSchrift'].replace(['Deutsch'],1)\n",
    "# Eigsprache: deutsch=1, nicht deutsch=0\n",
    "umfrage['eigSprache'] = umfrage['eigSprache'].replace([np.nan,'Russisch','Serbisch','Englisch','Italienisch','Vietnamesisch','Persisch','Kroatisch','Polnisch','Kurdisch','Arabisch','Griechisch','Spanisch','Niederl&auml;ndisch','T&uuml;rkisch','eine andere, und zwar...'],0)\n",
    "umfrage['eigSprache'] = umfrage['eigSprache'].replace(['Deutsch'],1)\n",
    "# Sprache mutter: deutsch = 1, nicht deutsch = 0\n",
    "umfrage['SpracheMutter'] = umfrage['SpracheMutter'].replace([np.nan,'Russisch','Serbisch','Englisch','Italienisch','Vietnamesisch','Persisch','Kroatisch','Polnisch','Kurdisch','Arabisch','Griechisch','Spanisch','Niederl&auml;ndisch','T&uuml;rkisch','eine andere, und zwar...'],0)\n",
    "umfrage['SpracheMutter'] = umfrage['SpracheMutter'].replace(['Deutsch'],1)\n",
    "# Sprache vater: deutsch = 1, nicht deutsch = 0\n",
    "umfrage['SpracheVater'] = umfrage['SpracheVater'].replace([np.nan,'Russisch','Serbisch','Englisch','Italienisch','Vietnamesisch','Persisch','Kroatisch','Polnisch','Kurdisch','Arabisch','Griechisch','Spanisch','Niederl&auml;ndisch','T&uuml;rkisch','eine andere, und zwar...'],0)\n",
    "umfrage['SpracheVater'] = umfrage['SpracheVater'].replace(['Deutsch'],1)\n",
    "#Abieltern: weiÃŸnicht = nan\n",
    "umfrage['AbiEltern'] = umfrage['AbiEltern'].replace(['weissnicht'],np.nan)\n",
    "#Buecher\n",
    "umfrage['Buecher'] = umfrage['Buecher'].replace(['10'],0)\n",
    "umfrage['Buecher'] = umfrage['Buecher'].replace(['50'],1)\n",
    "umfrage['Buecher'] = umfrage['Buecher'].replace(['100'],2)\n",
    "umfrage['Buecher'] = umfrage['Buecher'].replace(['200'],3)\n",
    "\n",
    "relevant_cols = [  'SchreibenGern', 'SchreibenLeicht','deutschnote',\n",
    "       'eigSprache', 'ErstSchrift', 'SpracheMutter', 'LesenGern',\n",
    "       'SpracheVater', 'LesenLeicht']#'AbiEltern'\n",
    "\n",
    "for i in relevant_cols:\n",
    "    umfrage[i] = umfrage[i].astype('float32')\n",
    "    umfrage[i] = umfrage[i].fillna(umfrage[i].mean())\n",
    "\n",
    "umfrage = umfrage[['UserID','AbiEltern','Buecher','eigSprache']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import preprocessed data\n",
    "infile = open('../04_predictionUserHistory/01_data/final_data_preprocessed.pkl','rb')\n",
    "import_file = pickle.load(infile)\n",
    "infile.close()\n",
    "smaller_features=import_file[['UserID','Erstloesung','Erfolg','Schwierigkeit','Wochentag','ist_Schulzeit','MehrfachFalsch','Testposition__FT', 'Testposition__nt', 'Testposition__pruefung',\n",
    "       'Testposition__training', 'Testposition__version', 'Testposition__vt',\n",
    "       'Testposition__zt', 'beendet', 'Fehler', 'HA__HA', 'HA__Self', 'HA__nt',\n",
    "       'HA__vt', 'HA__zt', 'Klassenstufe', 'Jahredabei',\n",
    "       'Sex__m', 'Sex__w']]\n",
    "\n",
    "# keep only users who took part in survey\n",
    "userIds = list(umfrage['UserID'])\n",
    "smaller_features = smaller_features[smaller_features['UserID'].isin(userIds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import all sentences\n",
    "infile = open(path+'saetze.pkl','rb')\n",
    "saetze = pickle.load(infile)\n",
    "infile.close()\n",
    "saetze = saetze[['satzID','Schwierigkeit','Art','AufgabenID']]\n",
    "\n",
    "#Art: only GK\n",
    "options = ['GK'] \n",
    "saetze = saetze[saetze['Art'].isin(options)] \n",
    "\n",
    "#Aufgaben ID : all except 0 (0 is kompetenztests) \n",
    "options = ['0'] \n",
    "saetze = saetze[~saetze['AufgabenID'].isin(options)] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Engineer historical learning data for each user who took part in the survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create empty df with null values\n",
    "# rows: all userIDs from preprocessed dataset\n",
    "# columns: all satzIDs\n",
    "\n",
    "userIDs = smaller_features[['UserID']]\n",
    "userIDs = userIDs.drop_duplicates()\n",
    "index_col = userIDs['UserID'].tolist()\n",
    "satzList = saetze[\"satzID\"].tolist()\n",
    "df = pd.DataFrame(0, index =index_col,columns =satzList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import xmlsaetze and xmlsaetze archiv to get historical data\n",
    "infile = open(path+'xmlsaetze.pkl','rb')\n",
    "xmlsaetze = pickle.load(infile)\n",
    "infile.close()\n",
    "xmlsaetze = xmlsaetze[['ID','UserID','UebungsID','SatzID','Erfolg','Datum']]\n",
    "\n",
    "infile = open(path+'xmlsaetze_archiv.pkl','rb')\n",
    "xmlsaetze_archiv = pickle.load(infile)\n",
    "infile.close()\n",
    "xmlsaetze_archiv = xmlsaetze_archiv[['ID','UserID','UebungsID','SatzID','Erfolg','Datum']]\n",
    "\n",
    "xmlsaetze = xmlsaetze.append(xmlsaetze_archiv)\n",
    "xmlsaetze_small = xmlsaetze[['UserID','SatzID','Erfolg','Datum']]\n",
    "\n",
    "# filter satzID: only GK and not kompetenztests\n",
    "options = saetze['satzID'].tolist()\n",
    "xmlsaetze_small = xmlsaetze_small[xmlsaetze_small['SatzID'].isin(options)] \n",
    "\n",
    "# filter userID: only User which appear in preprocessed dataset\n",
    "options = index_col\n",
    "xmlsaetze_userid = xmlsaetze_small[xmlsaetze_small['UserID'].isin(options)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add column last_login\n",
    "# not neccessary in production: in production we use t-3months \n",
    "# neccessary for training\n",
    "last_login = xmlsaetze_userid[['UserID','Datum']].groupby(['UserID']).max()\n",
    "last_login = pd.DataFrame(last_login.reset_index())\n",
    "last_login = last_login.rename(columns={\"Datum\": \"last_login\"})\n",
    "\n",
    "# merge last login \n",
    "xmlsaetze_userid_last_login= xmlsaetze_userid.merge(last_login, on='UserID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## iterate through xmlsaetze to finde historical data for each user\n",
    "for index, row in xmlsaetze_userid_last_login.iterrows():\n",
    "    user_cell = row['userid']\n",
    "    satz_cell = row['satz']\n",
    "    erfolg_cell = row['erfolg']\n",
    "    datum_cell = row['datum']\n",
    "    last_login_cell = row['last_login']\n",
    "\n",
    "    dateOfExercise = datetime.fromisoformat(str(datum_cell)).date()\n",
    "    dateOfLastLogin = datetime.fromisoformat(str(last_login_cell)).date()\n",
    "    acceptedDate = dateOfLastLogin + pd.DateOffset(months=-3)# accepted date calculates the date of the last login minus 3 months\n",
    "\n",
    "    #check if sentence and user is in df\n",
    "    if satz_cell in df.columns and user_cell in df.index and dateOfExercise > acceptedDate:\n",
    "        df.at[user_cell, satz_cell] = 1 if erfolg_cell == 1 else -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## join historical data to dataset and to survey\n",
    "historical_join = df.reset_index()\n",
    "historical_join = historical_join.rename(columns={\"index\": \"UserID\"})\n",
    "historical_join.to_pickle('historicalJoin_3months.pkl')\n",
    "\n",
    "dataset = pd.merge(smaller_features, historical_join, how='left', on='UserID')\n",
    "dataset.to_pickle('smallSampleSet_3months.pkl')\n",
    "\n",
    "df = pd.merge(dataset, umfrage, on='UserID')\n",
    "df.to_pickle('preprocessed_umfrage.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### SUBGROUPING\n",
    "\n",
    "# Abi eltern\n",
    "df.AbiEltern = df.AbiEltern.astype('float')\n",
    "df_abi = df[df.AbiEltern > 0]\n",
    "df_keinAbi = df[df.AbiEltern ==0]\n",
    "df_weissnicht = df[df.AbiEltern.isnull()]\n",
    "\n",
    "print(len(df_abi.UserID.unique()))\n",
    "print(len(df_keinAbi.UserID.unique()))\n",
    "print(len(df_weissnicht.UserID.unique()))\n",
    "\n",
    "#Gender\n",
    "df_boys = df[df.Sex__m == 1]\n",
    "df_girls = df[df.Sex__w == 1]\n",
    "\n",
    "print(len(df_boys.UserID.unique()))\n",
    "print(len(df_girls.UserID.unique()))\n",
    "\n",
    "# Migration\n",
    "df_deutsch = df[df.eigSprache == 1]\n",
    "df_migration = df[df.eigSprache == 0]\n",
    "\n",
    "print(len(df_deutsch.UserID.unique()))\n",
    "print(len(df_migration.UserID.unique()))\n",
    "\n",
    "# Anzahl BÃ¼cher\n",
    "df['Buecher'] = df['Buecher'].replace({'10':0, '200':1})\n",
    "df_buch0 = df[df.Buecher == 0.0]\n",
    "df_buch1 = df[df.Buecher == 1]\n",
    "\n",
    "print(len(df_buch0.UserID.unique()))\n",
    "print(len(df_buch1.UserID.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save subgroups to pickle files\n",
    "df_abi.to_pickle('double_df_abi.pkl')\n",
    "df_keinAbi.to_pickle('double_df_keinAbi.pkl')\n",
    "df_weissnicht.to_pickle('double_df_weissnicht.pkl')\n",
    "df_boys.to_pickle('double_df_boys.pkl')\n",
    "df_girls.to_pickle('double_df_girls.pkl')\n",
    "df_deutsch.to_pickle('double_df_deutsch.pkl')\n",
    "df_migration.to_pickle('double_df_migration.pkl')\n",
    "df_buch0.to_pickle('double_df_buch0.pkl')\n",
    "df_buch1.to_pickle('double_df_buch1.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b31c238c9fc47863b962b845562f9bed824c7de75300565f12a0ac751e3044d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
